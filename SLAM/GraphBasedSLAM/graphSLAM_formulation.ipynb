{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "\n",
    "Let a robot's trajectory through its environment be represented by a sequence of $N$ poses: $\\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_N$.  Each pose lies on a manifold: $\\mathbf{p}_i \\in \\mathcal{M}$.  Simple examples of manifolds used in Graph SLAM include 1-D, 2-D, and 3-D space, i.e., $\\mathbb{R}$, $\\mathbb{R}^2$, and $\\mathbb{R}^3$.  These environments are *rectilinear*, meaning that there is no concept of orientation.  By contrast, in $SE(2)$ problem settings a robot's pose consists of its location in $\\mathbb{R}^2$ and its orientation $\\theta$.  Similarly, in $SE(3)$ a robot's pose consists of its location in $\\mathbb{R}^3$ and its orientation, which can be represented via Euler angles, quaternions, or $SO(3)$ rotation matrices.  \n",
    "\n",
    "As the robot explores its environment, it collects a set of $M$ measurements $\\mathcal{Z} = \\{\\mathbf{z}_j\\}$.  Examples of such measurements include odometry, GPS, and IMU data.  Given a set of poses $\\mathbf{p}_1, \\ldots, \\mathbf{p}_N$, we can compute the estimated measurement $\\hat{\\mathbf{z}}_j(\\mathbf{p}_1, \\ldots, \\mathbf{p}_N)$.  We can then compute the *residual* $\\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j)$ for measurement $j$.  The formula for the residual depends on the type of measurement.  As an example, let $\\mathbf{z}_1$ be an odometry measurement that was collected when the robot traveled from $\\mathbf{p}_1$ to $\\mathbf{p}_2$.  The expected measurement and the residual are computed as\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\mathbf{z}}_1(\\mathbf{p}_1, \\mathbf{p}_2) &= \\mathbf{p}_2 \\ominus \\mathbf{p}_1 \\\\\n",
    "    \\mathbf{e}_1(\\mathbf{z}_1, \\hat{\\mathbf{z}}_1) &= \\mathbf{z}_1 \\ominus \\hat{\\mathbf{z}}_1 = \\mathbf{z}_1 \\ominus (\\mathbf{p}_2 \\ominus \\mathbf{p}_1),\n",
    "\\end{align}\n",
    "\n",
    "where the $\\ominus$ operator indicates inverse pose composition.  We model measurement $\\mathbf{z}_j$ as having independent Gaussian noise with zero mean and covariance matrix $\\Omega_j^{-1}$; we refer to $\\Omega_j$ as the *information matrix* for measurement $j$.  That is,\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mathbf{z}_j \\ | \\ \\mathbf{p}_1, \\ldots, \\mathbf{p}_N) = \\eta_j \\exp \\left( (-\\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j))^T \\Omega_j \\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta_j$ is the normalization constant.\n",
    "\n",
    "The objective of Graph SLAM is to find the maximum likelihood set of poses given the measurements $\\mathcal{Z} = \\{\\mathbf{z}_j\\}$; in other words, we want to find \n",
    "\n",
    "\\begin{equation}\n",
    "    \\argmax_{\\mathbf{p}_1, \\ldots, \\mathbf{p}_N} p(\\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\ | \\ \\mathcal{Z}) \n",
    "\\end{equation}\n",
    "\n",
    "Using Bayes' rule, we can write this probability as\n",
    "\n",
    "\\begin{align}\n",
    "    p(\\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\ | \\ \\mathcal{Z}) &= \\frac{p( \\mathcal{Z} \\ | \\ \\mathbf{p}_1, \\ldots, \\mathbf{p}_N) p(\\mathbf{p}_1, \\ldots, \\mathbf{p}_N) }{ p(\\mathcal{Z}) } \\\\\n",
    "    &\\propto p( \\mathcal{Z} \\ | \\ \\mathbf{p}_1, \\ldots, \\mathbf{p}_N),\n",
    "\\end{align}\n",
    "\n",
    "since $p(\\mathcal{Z})$ is a constant (albeit, an unknown constant) and we assume that $p(\\mathbf{p}_1, \\ldots, \\mathbf{p}_N)$ is uniformly distributed.  Therefore, we can simplify the Graph SLAM optimization as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    \\underset{\\mathbf{p}_1, \\ldots, \\mathbf{p}_N}{\\operatorname{arg\\,max}} p(\\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\ | \\ \\mathcal{Z}) &= \\argmax_{\\mathbf{p}_1, \\ldots, \\mathbf{p}_N} p( \\mathcal{Z} \\ | \\ \\mathbf{p}_1, \\ldots, \\mathbf{p}_N) \\\\\n",
    "    &= \\underset{\\mathbf{p}_1, \\ldots, \\mathbf{p}_N}{\\operatorname{arg\\,max}} \\prod_{j=1}^M p(\\mathbf{z}_j \\ | \\ \\mathbf{p}_1, \\ldots, \\mathbf{p}_N) \\\\\n",
    "    &= \\underset{\\mathbf{p}_1, \\ldots, \\mathbf{p}_N}{\\operatorname{arg\\,max}} \\prod_{j=1}^M \\exp \\left( -(\\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j))^T \\Omega_j \\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j) \\right) \\\\\n",
    "    &= \\underset{\\mathbf{p}_1, \\ldots, \\mathbf{p}_N}{\\operatorname{arg\\,min}} \\sum_{j=1}^M (\\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j))^T \\Omega_j \\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j).\n",
    "\\end{align}\n",
    "\n",
    "We define\n",
    "\n",
    "\\begin{equation}\n",
    "    \\chi^2 := \\sum_{j=1}^M (\\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j))^T \\Omega_j \\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j),\n",
    "\\end{equation}\n",
    "\n",
    "and this is what we seek to minimize.\n",
    "\n",
    "\n",
    "### Dimensionality and Pose Representation\n",
    "\n",
    "Before proceeding further, it is helpful to discuss the dimensionality of the problem.  We have:\n",
    "\n",
    "* A set of $N$ poses $\\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_N$, where each pose lies on the manifold $\\mathcal{M}$\n",
    "\n",
    "  * Each pose $\\mathbf{p}_i$ is represented as a vector in (a subset of) $\\mathbb{R}^d$.  For example:\n",
    "    * An $SE(2)$ pose is typically represented as $(x, y, \\theta)$, and thus $d = 3$.\n",
    "    * An $SE(3)$ pose is typically represented as $(x, y, z, q_x, q_y, q_z, q_w)$, where $(x, y, z)$ is a point in $\\mathbb{R}^3$ and $(q_x, q_y, q_z, q_w)$ is a *quaternion*, and so $d = 7$.\n",
    "  * We also need to be able to represent each pose compactly as a vector in (a subset of) $\\mathbb{R}^c$.\n",
    "    * Since an $SE(2)$ pose has three degrees of freedom, the $(x, y, \\theta)$ representation is again sufficient and $c=3$.  \n",
    "    * An $SE(3)$ pose only has six degrees of freedom, and we can represent it compactly as $(x, y, z, q_x, q_y, q_z)$, and thus $c=6$.\n",
    "  * We use the $\\boxplus$ operator to indicate pose composition when one or both of the poses are represented compactly.  The output can be a pose in $\\mathcal{M}$ or a vector in $\\mathbb{R}^c$, as required by context.\n",
    "\n",
    "* A set of $M$ measurements $\\mathcal{Z} = \\{\\mathbf{z}_1, \\mathbf{z}_2, \\ldots, \\mathbf{z}_M\\}$\n",
    "\n",
    "  * Each measurement's dimensionality can be unique, and we will use $\\bullet$ to denote a \"wildcard\" variable.\n",
    "  * Measurement $\\mathbf{z}_j \\in \\mathbb{R}^\\bullet$ has an associated information matrix $\\Omega_j \\in \\mathbb{R}^{\\bullet \\times \\bullet}$ and residual function $\\mathbf{e}_j(\\mathbf{z}_j, \\hat{\\mathbf{z}}_j) = \\mathbf{e}_j(\\mathbf{z}_j, \\mathbf{p}_1, \\ldots, \\mathbf{p}_N) \\in \\mathbb{R}^\\bullet$.\n",
    "  * A measurement could, in theory, constrain anywhere from 1 pose to all $N$ poses.  In practice, each measurement usually constrains only 1 or 2 poses.  \n",
    "\n",
    "\n",
    "### Graph SLAM Algorithm\n",
    "\n",
    "The \"Graph\" in Graph SLAM refers to the fact that we view the problem as a graph.  The graph has a set $\\mathcal{V}$ of $N$ vertices, where each vertex $v_i$ has an associated pose $\\mathbf{p}_i$.  Similarly, the graph has a set $\\mathcal{E}$ of $M$ edges, where each edge $e_j$ has an associated measurement $\\mathbf{z}_j$.  In practice, the edges in this graph are either unary (i.e., a loop) or binary.  (Note: $e_j$ refers to the edge in the graph associated with measurement $\\mathbf{z}_j$, whereas $\\mathbf{e}_j$ refers to the residual function associated with $\\mathbf{z}_j$.)\n",
    "\n",
    "We want to optimize\n",
    "\n",
    "\\begin{equation}\n",
    "    \\chi^2 = \\sum_{e_j \\in \\mathcal{E}} \\mathbf{e}_j^T \\Omega_j \\mathbf{e}_j.\n",
    "\\end{equation}\n",
    "\n",
    "Let $\\mathbf{x}_i \\in \\mathbb{R}^c$ be the compact representation of pose $\\mathbf{p}_i \\in \\mathcal{M}$, and let\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x} := \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\end{bmatrix} \\in \\mathbb{R}^{cN}\n",
    "\\end{equation}\n",
    "\n",
    "We will solve this optimization problem iteratively.  Let\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}^{k+1} := \\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k = \\begin{bmatrix} \\mathbf{x}_1 \\boxplus \\Delta \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\boxplus \\Delta \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\boxplus \\Delta \\mathbf{x}_2 \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The $\\chi^2$ error at iteration $k+1$ is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\chi_{k+1} = \\sum_{e_j \\in \\mathcal{E}} \\underbrace{\\left[ \\mathbf{e}_j(\\mathbf{x}^{k+1}) \\right]^T}_{1 \\times \\bullet} \\underbrace{\\Omega_j}_{\\bullet \\times \\bullet} \\underbrace{\\mathbf{e}_j(\\mathbf{x}^{k+1})}_{\\bullet \\times 1}.\n",
    "\\end{equation}\n",
    "\n",
    "We will linearize the errors as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{e}_j(\\mathbf{x}^{k+1}) &= \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k) \\\\\n",
    "    &\\approx \\mathbf{e}_j(\\mathbf{x}^{k}) + \\frac{\\partial}{\\partial \\Delta \\mathbf{x}^k} \\left[ \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k) \\right] \\Delta \\mathbf{x}^k \\\\\n",
    "    &= \\mathbf{e}_j(\\mathbf{x}^{k}) + \\left( \\left. \\frac{\\partial \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)} \\right|_{\\Delta \\mathbf{x}^k = \\mathbf{0}} \\right) \\frac{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial \\Delta \\mathbf{x}^k} \\Delta \\mathbf{x}^k.\n",
    "\\end{align}\n",
    "\n",
    "Plugging this into the formula for $\\chi^2$, we get:\n",
    "\n",
    "\\begin{align}\n",
    "    \\chi_{k+1}^2 &\\approx \\ \\ \\ \\ \\ \\sum_{e_j \\in \\mathcal{E}} \\underbrace{[ \\mathbf{e}_j(\\mathbf{x}^k)]^T}_{1 \\times \\bullet} \\underbrace{\\Omega_j}_{\\bullet \\times \\bullet} \\underbrace{\\mathbf{e}_j(\\mathbf{x}^k)}_{\\bullet \\times 1} \\\\\n",
    "    &\\hphantom{\\approx} \\ \\ \\ + \\sum_{e_j \\in \\mathcal{E}} \\underbrace{[ \\mathbf{e}_j(\\mathbf{x^k}) ]^T }_{1 \\times \\bullet} \\underbrace{\\Omega_j}_{\\bullet \\times \\bullet} \\underbrace{\\left( \\left. \\frac{\\partial \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)} \\right|_{\\Delta \\mathbf{x}^k = \\mathbf{0}} \\right)}_{\\bullet \\times dN} \\underbrace{\\frac{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial \\Delta \\mathbf{x}^k}}_{dN \\times cN} \\underbrace{\\Delta \\mathbf{x}^k}_{cN \\times 1} \\\\\n",
    "    &\\hphantom{\\approx} \\ \\ \\ + \\sum_{e_j \\in \\mathcal{E}} \\underbrace{(\\Delta \\mathbf{x}^k)^T}_{1 \\times cN} \\underbrace{ \\left( \\frac{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial \\Delta \\mathbf{x}^k} \\right)^T}_{cN \\times dN} \\underbrace{\\left( \\left. \\frac{\\partial \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)} \\right|_{\\Delta \\mathbf{x}^k = \\mathbf{0}} \\right)^T}_{dN \\times \\bullet} \\underbrace{\\Omega_j}_{\\bullet \\times \\bullet} \\underbrace{\\left( \\left. \\frac{\\partial \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)} \\right|_{\\Delta \\mathbf{x}^k = \\mathbf{0}} \\right)}_{\\bullet \\times dN} \\underbrace{\\frac{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial \\Delta \\mathbf{x}^k}}_{dN \\times cN} \\underbrace{\\Delta \\mathbf{x}^k}_{cN \\times 1} \\\\\n",
    "    &= \\chi_k^2 + 2 \\mathbf{b}^T \\Delta \\mathbf{x}^k + (\\Delta \\mathbf{x}^k)^T H \\Delta \\mathbf{x}^k,\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{b}^T &= \\sum_{e_j \\in \\mathcal{E}} \\underbrace{[ \\mathbf{e}_j(\\mathbf{x^k}) ]^T }_{1 \\times \\bullet} \\underbrace{\\Omega_j}_{\\bullet \\times \\bullet} \\underbrace{\\left( \\left. \\frac{\\partial \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)} \\right|_{\\Delta \\mathbf{x}^k = \\mathbf{0}} \\right)}_{\\bullet \\times dN} \\underbrace{\\frac{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial \\Delta \\mathbf{x}^k}}_{dN \\times cN} \\\\\n",
    "    H &= \\sum_{e_j \\in \\mathcal{E}} \\underbrace{ \\left( \\frac{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial \\Delta \\mathbf{x}^k} \\right)^T}_{cN \\times dN} \\underbrace{\\left( \\left. \\frac{\\partial \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)} \\right|_{\\Delta \\mathbf{x}^k = \\mathbf{0}} \\right)^T}_{dN \\times \\bullet} \\underbrace{\\Omega_j}_{\\bullet \\times \\bullet} \\underbrace{\\left( \\left. \\frac{\\partial \\mathbf{e}_j(\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)} \\right|_{\\Delta \\mathbf{x}^k = \\mathbf{0}} \\right)}_{\\bullet \\times dN} \\underbrace{\\frac{\\partial (\\mathbf{x}^k \\boxplus \\Delta \\mathbf{x}^k)}{\\partial \\Delta \\mathbf{x}^k}}_{dN \\times cN}.\n",
    "\\end{align}\n",
    "\n",
    "Using this notation, we obtain the optimal update as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Delta \\mathbf{x}^k = -H^{-1} \\mathbf{b}.\n",
    "\\end{equation}\n",
    "\n",
    "We apply this update to the poses and repeat until convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
